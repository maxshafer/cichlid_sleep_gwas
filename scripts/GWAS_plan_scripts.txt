# New methods for GATK + Filtering Variants
# 
# Two big takeaways from meeting with Milan:
# 
# 1) GATK can be sped up by breaking up the genome into pieces, and running parallel jobs on shorter queues for each. This is most important for HaploTypeCaller, GenoGVCF (this most of all), and VarFiltration steps.
# 	- This is accomplished by using 'SplitIntervals' to generate the list of equally sized intervals
# 	- -L option allows you to use them for GATK commands
# 	- He runs a for loop (probably to submit batch jobs?) for each interval (maybe it's a for loop that runs an sbatch with an argument (the interval) for each sample or vice versa). Alternatively, you can index a file with the two variables (sample and interval) and just run an array of 1:total_jobs
# 	- Would have 1 job for each interval (40 for HaploTypeCaller or 1000 for GenoTypeGVCF), for each sample
# 	- 'GatherGVCFs' and 'CombineGVCF' are commands that will recombine the files into one, which then needs to be indexed with 'IndexFeatureFile'
# 	
# 2) Instead of filtering variants with GATK's tools, Milan creates several overlapping genome masks (4), which he uses as filtering
# 	- (1) Mappability mask (he gave this to me for old genome version) - generated by mapping genomic kmers back to genome, then determining which ones don't map well (some cutoff)
# 	- (2) Read depth mask - excluding regions with too little or too much read depth (based on histogram of read depths)
# 	- (3) LowQual filter - this is basically just filtering out what GATK calls LowQual (sometimes he also takes any bp that are adjacent to LowQual bps by GATK)
# 	- (4) Filters based on missing data - basically, any bp that has missing data, excludes it (might not be wise, can always check this later or filter downstream)
# 	- These 4 masks are combined into the Union of all, and used with -mask option in 'SelectVariants' (after running 'VariantFiltration')
# 	
# Finally, he also suggested installing versions of software into my own drive, so that I can always continue to use the same installation of software over time. Alternatively, always update to match current softwares.
# 
# Gameplan:
# 
# 1) Rerun steps 0-5 using old genome annotation, so I can use his mappability mask, and compare with Carolin's results. Will need to update to GATK version 4.X (maybe also install this in my folder? And run from there). Make sure to Update prefetch script to include latest genomes?
# 	- OK, installing GATK super easy, but it seems I need to load a java module ('module load Java' works) and point it to my jar file (/scicore/home/schiera/gizevo30/gatk_4.2.4.1/gatk-4.2.4.1/gatk)
# 2) Figure out SplitInvervals and how best to run the complex array job (of two options)
# 3) Work out the read depth mask (will need variant files for this)



# Lets define intervals using SplitIntervals

# Create Dictionary file for reference fasta
~/gatk-4.2.4.1/gatk CreateSequenceDictionary -R GCF_001858045.1_ASM185804v2_genomic_edit.fna

# Create index file for reference fasta
samtools faidx GCF_001858045.1_ASM185804v2_genomic_edit.fna

# bwa index also?

bwa index GCF_001858045.1_ASM185804v2_genomic_edit.fna


~/gatk-4.2.4.1/gatk SplitIntervals -R GCF_001858045.1_ASM185804v2_genomic_edit.fna -scatter 40 -O intervals_ASM185804v2_40x

~/gatk-4.2.4.1/gatk SplitIntervals -R GCF_001858045.1_ASM185804v2_genomic_edit.fna -scatter 1000 -O intervals_ASM185804v2_1000x 





# Interval lists look like this and are zero indexed

0000-scattered.interval_list
...
0039-scattered.interval_list

# And submited to gatk tools with the -L option




## Haplotype caller (old command):

java -jar $EBROOTGATK/GenomeAnalysisTK.jar -T HaplotypeCaller 
	-R /scicore/home/schiera/gizevo30/projects/cichlids_2/genome/GCF_001858045.2_O_niloticus_UMD_NMBU_genomic.fna 
	-I SRR96${SLURM_ARRAY_TASK_ID}_6_realign.bam 
	-nct 4 
	-ERC GVCF 
	-minPruning 1 
	-minDanglingBranchLength 4 
	--max_alternate_alleles 10 
	--genotyping_mode DISCOVERY 
	-o SRR96${SLURM_ARRAY_TASK_ID}_variants.g.vcf.gz
	
	

~/gatk-4.2.4.1/gatk -T HaplotypeCaller 
	-R ~/projects/cichlids_2/genome/GCF_001858045.2_O_niloticus_UMD_NMBU_genomic.fna
	-L 0000-scattered.interval_list
	-I SRR96${SLURM_ARRAY_TASK_ID}_6_realign.bam
	-ERC GVCF
	-o SRR96${SLURM_ARRAY_TASK_ID}_0000_variants.g.vcf.gz


# GatherVcfs to recombine intervals, then IndexFeatureFile (same script)

# CombineGVCFs to combine per-sample gvfcs, then run GenoGVCF with ~1000 intervals



## This works, very quickly for ~5samples, would think it grows much longer for more samples (4 min total wall time, including pending nodes for 1k jobs on the 6hr)

~/gatk-4.2.4.1/gatk GenotypeGVCFs 
		-R ~/projects/cichlids_2/genome/GCF_001858045.1_ASM185804v2_genomic_edit.fna
		-V cohort.g.vcf.gz
		-O cohort_geno.g.vcf.gz
		-L /scicore/home/schiera/gizevo30/projects/cichlids_2/genome/intervals_ASM185804v2_1000x/${INTERVAL}



####################################################################################
####################################################################################


## OK GenotypeGVCFs is done, couple hundred Gs size of vcf
## Need to get 3 filters (won't do 4 for now)

# - (1) Mappability mask (he gave this to me for old genome version) - generated by mapping genomic kmers back to genome, then determining which ones don't map well (some cutoff)
# - (2) Read depth mask - excluding regions with too little or too much read depth (based on histogram of read depths)
# - (3) LowQual filter - this is basically just filtering out what GATK calls LowQual (sometimes he also takes any bp that are adjacent to LowQual bps by GATK)
# - (4) Filters based on missing data - basically, any bp that has missing data, excludes it (might not be wise, can always check this later or filter downstream)

## Mask #1

/scicore/home/schiera/gizevo30/projects/cichlids_2/masks/mapability_mask_100_90.bed

## For mask #2
## Getting (overall) depth from a VCF:

# ~/programs/bcftools-1.11/bcftools query -f '%INFO/DP\n' ${s}_region${r}_genotypedAllSites.vcf.gz | awk '{if (rand() < 0.1) { if ($1 == ".") {$1 = 0;} print;}}' | gzip -c > ${s}_region${r}_genotypedAllSites.DP.gz

module load BCFtools/1.12-GCC-10.3.0

bcftools query -f '%INFO/DP\n' cohort_geno_gathered.g.vcf.gz | awk '{if (rand() < 0.1) { if ($1 == ".") {$1 = 0;} print;}}' | gzip -c > cohort_geno_gathered.g.vcf.gz

## Maybe do this over the intervals? would be much faster probably (1/1000 of the data for bcftools and awk to go through)
## This takes a random subset of nucleotide positions, and outputs the read depth. Use this to ID a good range of depths to use (look for the normal distribution)

# bcftools query -f '%INFO/DP\n' cohort_geno_0000-scattered.interval_list.g.vcf.gz | awk '{if (rand() < 0.1) { if ($1 == ".") {$1 = 0;} print;}}' | gzip -c > cohort_geno_0000-scattered.interval_list.DP.gz

bcftools query -f '%INFO/DP\n' cohort_geno_gathered.g.vcf.gz | awk '{if (rand() < 0.1) { if ($1 == ".") {$1 = 0;} print;}}' | gzip -c > cohort_geno_gathered.DP.gz

# So the full thing runs, but takes a couple of hours in an srun - but looks good, similar to what he sees I guess, would choose something like 900/1000 - 1800/1900


## Using depth limits to create a bed file with regions outside of these limits:

#!/bin/bash
  
# r=$1
# s=$2
# min=$3
# max=$4

# ~/programs/bcftools-1.11/bcftools query -f '%CHROM\t%POS\t%INFO/DP\n' ${s}_region${r}_genotypedAllSites.vcf.gz | awk -v mi="$min" -v ma="$max" '{if ($3 == ".") {$3 = 0;} if (($3 <= mi) || ($3 >= ma)) {print $1"\t"$2-1"\t"$2;}}' > ${s}_region${r}_genotypedAllSites.DPmask.tmp.bed
# ~/programs/bedtools-2.17.0/bin/mergeBed -i ${s}_region${r}_genotypedAllSites.DPmask.tmp.bed > ${s}_region${r}_genotypedAllSites.DPmask.bed

min=900
max=1900

bcftools query -f '%CHROM\t%POS\t%INFO/DP\n' cohort_geno_gathered.g.vcf.gz | awk -v mi="$min" -v ma="$max" '{if ($3 == ".") {$3 = 0;} if (($3 <= mi) || ($3 >= ma)) {print $1"\t"$2-1"\t"$2;}}' > ../masks/cohort_geno_gathered.DPmask.tmp.bed

module load BEDTools/2.30.0-GCC-10.3.0

# UNIX command for sort seems to work, and allow mergeBed to run, but the result is smaller - not sure why? Maybe there were some duplicated regions/bp - or it combines adjacent bps into ranges!

sort -k 1,1 -k2,2n cohort_geno_gathered.DPmask.tmp.bed > cohort_geno_gathered.DPmask.tmp2.bed

mergeBed -i cohort_geno_gathered.DPmask.tmp2.bed > cohort_geno_gathered.DPmask.bed

/scicore/home/schiera/gizevo30/projects/cichlids_2/masks/cohort_geno_gathered.DPmask.bed

## For mask #3
## LowQual filter (what GATK calls lowqual)
## Very similar to above, just searching a different column for a different character string

# Getting a bed file for the GATK LowQual annotation:
# #!/bin/bash
#   
# i=$1
# s=$2
# 
# gunzip -c ${s}_region${i}_genotypedAllSites.vcf.gz | awk '{if ($7=="LowQual") { print $1"\t"$2-1"\t"$2; }}' > ${s}_region${i}_genotypedAllSites.LQ.tmp.bed
# ~/programs/bedtools-2.17.0/bin/mergeBed -i ${s}_region${i}_genotypedAllSites.LQ.tmp.bed > ${s}_region${i}_genotypedAllSites.LQ.bed

# This outputs the bed file
gunzip -c cohort_geno_gathered.g.vcf.gz | awk '{if ($7=="LowQual") { print $1"\t"$2-1"\t"$2; }}' > ../masks/cohort_geno_gathered.g.vcf.gz.LQ.tmp.bed
# This indexes it?
~/programs/bedtools-2.17.0/bin/mergeBed -i ${s}_region${i}_genotypedAllSites.LQ.tmp.bed > ${s}_region${i}_genotypedAllSites.LQ.bed

# There doesn't seem to be any "LowQual" bps, maybe they were removed by GenotypeGVCF? The file before GenotypeGVCF was ~500Gb, and after only 211Gb - maybe already removed? Should I rerun this?


# Try running it on the pre-GenotypeGVCF file (larger)

gunzip -c cohort.g.vcf.gz | awk '{if ($7=="LowQual") { print $1"\t"$2-1"\t"$2; }}' > ../masks/cohort.g.vcf.gz.LQ.tmp.bed

# The following finds 1 line, and that's in the header - so it doesn't seem there are any, so Milan's line works, but there are none.
# I'm not sure if GenotypeGCVF is removing them, but the file after is definitely smaller
gunzip -c ../sra_reads_nobackup/cohort_geno_gathered.g.vcf.gz | grep "LowQual" > test.txt

# ##FILTER=<ID=LowQual,Description="Low quality">


## It seems maybe the LowQual filter is applied based on the filters you give gatk ("soft filter")
## Ahh, I think these are supplied by "VariantFiltration", then you pass the masks to "SelectVariants"

## Need to run VariantFiltration first, with default parameters? But do these paramaters include a depth filter?

~/gatk-4.2.4.1/gatk VariantFiltration -R ~/projects/cichlids_2/genome/GCF_001858045.1_ASM185804v2_genomic_edit.fna -V ~/projects/cichlids_2/sra_reads_nobackup/cohort_geno_gathered.g.vcf.gz -O ~/projects/cichlids_2/sra_reads_nobackup/cohort_geno_gathered_filtered.g.vcf.gz


### OK, I need to index the result from GatherGVCF after Genotyping (for some reason, IndexFeatureFile doesn't work, it is not sorted?)
## First sort it, then index? Sort with bcftools
## If this works, I can add it to step8, before indexing

module load BCFtools/1.12-GCC-10.3.0
temp=/scicore/home/schiera/gizevo30/projects/cichlids_2/tmp_dir/
bcftools sort cohort_geno_gathered.g.vcf.gz -T ./temp -Oz -o cohort_geno_gathered_sorted.g.vcf.gz 

# this didn't actually use the temp drive I specified above... also was killed by cluster when trying to merge the resulting files (got to 11Gb before it died)

# Writing to ./temp
# Merging 1383 temporary files
# srun: Force Terminated job 33583213
# srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
# srun: error: shi101: task 0: Killed



# I ran on slurm, failed at writing 01215.bcf (of 1383?) - gave it more scratch memory, maybe it will run

# Can try indexing one of the intervals from GenotypeGVCF (that is gathered), and run the filtration on that?
# This works, so it's an issue with GatherGVCF, that is putting things out of order? Maybe the list is wrong? I can regather?

cohort_geno_0001-scattered.interval_list.g.vcf.gz

~/gatk-4.2.4.1/gatk VariantFiltration -R ~/projects/cichlids_2/genome/GCF_001858045.1_ASM185804v2_genomic_edit.fna -V ~/projects/cichlids_2/sra_reads_nobackup/cohort_interval_vcfs/cohort_geno_0001-scattered.interval_list.g.vcf.gz -O ~/projects/cichlids_2/sra_reads_nobackup/cohort_interval_vcfs/cohort_geno_0001-scattered.interval_list_filtered.g.vcf.gz

# Running with more memory failed on 01000.bcf, so it doestn' seem to make a difference??

# I'm trying GenotypeGVCF again, so I can rerun GatherGVCF and use the reference fasta (maybe this will put it in order?) If not, I can try the GenomicsDBImport pipeline (instead of CombineGVCF)
# Using the reference file I still get the same error (record's out of order)

##### AHHHHHHHH it's right at one of the 1k intervals (0002 goes to 3029643), so likely they are out of order because of that
##### I know that they can be indexed if using the 80 intervals, so maybe I can try the scatter gather using those for the GenotypeGVCF step.

A USER ERROR has occurred: Error while trying to create index for ../cohort_geno_gathered.g.vcf.gz. Error was: htsjdk.tribble.TribbleException.MalformedFeatureFile: Input file is not sorted by start position.
We saw a record with a start of NC_031965:3029642 after a record with a start of NC_031965:3029643, for input source: ../cohort_geno_gathered.g.vcf.gz


## OK now I will try the GenomicsDBImport version, instead of CombineGVCF
## I have all the g.vcf.gz files still
## It seems like you NEED to specify an interval for GenomicsDBImport, so I need to run this for each interval, then GenotypeGVCF on those intervals, then GatherGVCF on the results?
## This is slightly different, since I won't be scattering during Genotype, but still gathering after...

# Below seems to work, doesn't take long to do 1/1000 of the genome, but giving it 6hours to do 1/80

# comma separated df with rows, samples, interval
file_list="/scicore/home/schiera/gizevo30/projects/cichlids_2/scripts/index_array_80x.csv"

# this is the second column of index_array_40x.csv
INTERVAL=`sed -n "$SLURM_ARRAY_TASK_ID"p "${file_list}" | cut -f 3 -d ','`


~/gatk-4.2.4.1/gatk GenomicsDBImport --genomicsdb-workspace-path $INTERVAL --tmp-dir $TMPDIR -L /scicore/home/schiera/gizevo30/projects/cichlids_2/genome/intervals_ASM185804v2_80x/$INTERVAL  -V SRR9657475_variants.g.vcf.gz -V SRR9657476_variants.g.vcf.gz



## OK, will now need to try GenotypeGVCF, without intervals, but on the interval'd GenomicsDB
## Seems there are records with too many alleles??

# Chromosome NC_031966 position 24748916 (TileDB column 63121906) has too many alleles in the combined VCF record : 7 : current limit : 6. Fields, such as  PL, with length equal to the number of genotypes will NOT be added for this location.


# This seems to work now (after rerunning all the updated sample list)
# OK, so with the interval'd GenomicsDB, I run GenotypeGVCF on them, then gather the result
# Will then need to run Genotyping on each interval, followed by GatherGVCF to make the final vcf before filtering

~/gatk-4.2.4.1/gatk GenotypeGVCFs -R ~/projects/cichlids_2/genome/GCF_001858045.1_ASM185804v2_genomic_edit.fna -V gendb://$INTERVAL -O cohort_${INTERVAL}.g.vcf.gz --tmp-dir=$TMPDIR

## Needed to switch to gatk-4.2.4.0 instead, and now it can bypass the issue with more than 6 alleles
## Just downloaded and unpacked into my directory

~/gatk-4.2.4.0/gatk GenotypeGVCFs -R ~/projects/cichlids_2/genome/GCF_001858045.1_ASM185804v2_genomic_edit.fna -V gendb://$INTERVAL -O cohort_${INTERVAL}.g.vcf.gz --tmp-dir=$TMPDIR




## The old way of combining before Genotyping
~/gatk-4.2.4.1/gatk CombineGVCFs -R /scicore/home/schiera/gizevo30/projects/cichlids_2/genome/GCF_001858045.1_ASM185804v2_genomic_edit.fna -O cohort.g.vcf.gz -V SRR9657475_variants.g.vcf.gz -V SRR9657476_variants.g.vcf.gz

###################################################################################################
###################################################################################################

# Try again!!


## For mask #3
## LowQual filter (what GATK calls lowqual)
## Very similar to above, just searching a different column for a different character string

# Getting a bed file for the GATK LowQual annotation:
# #!/bin/bash
#   
# i=$1
# s=$2
# 
# gunzip -c ${s}_region${i}_genotypedAllSites.vcf.gz | awk '{if ($7=="LowQual") { print $1"\t"$2-1"\t"$2; }}' > ${s}_region${i}_genotypedAllSites.LQ.tmp.bed
# ~/programs/bedtools-2.17.0/bin/mergeBed -i ${s}_region${i}_genotypedAllSites.LQ.tmp.bed > ${s}_region${i}_genotypedAllSites.LQ.bed

# # This outputs the bed file
# gunzip -c cohort_geno_gathered.g.vcf.gz | awk '{if ($7=="LowQual") { print $1"\t"$2-1"\t"$2; }}' > ../masks/cohort_geno_gathered.g.vcf.gz.LQ.tmp.bed
# # This indexes it?
# ~/programs/bedtools-2.17.0/bin/mergeBed -i ${s}_region${i}_genotypedAllSites.LQ.tmp.bed > ${s}_region${i}_genotypedAllSites.LQ.bed

mergeBed -i ../masks/cohort_geno_gathered.g.vcf.gz.LQ.tmp.bed > ../masks/cohort_geno_gathered.g.vcf.gz.LQ.bed




# Merge multiple bed files:
# 
# #!/bin/bash
#   
# s=$1
# 
# cat ${s}_all_genotypedAllSites.DPmask.bed ${s}_all_genotypedAllSites.LQ.bed ${s}_all_genotypedAllSites.ANmask.bed /lustre/scratch119/humgen/projects/cichlid/seq/Tanganyika_new/sequibility/mapability_mask_100_90.bed | ~/programs/bedtools2/bin/sortBed -faidx /lustre/scratch119/humgen/projects/cichlid/seq/Tanganyika_new/GCF_001858045.1_ASM185804v2_genomic_edit.chrs -i stdin | ~/programs/bedtools2/bin/mergeBed -i stdin > ${s}_all_genotypedAllSites.allMasksMerged.bed

# Merge the 3 masks

cat cohort_geno_gathered.DPmask.bed cohort_geno_gathered.g.vcf.gz.LQ.bed mapability_mask_100_90.bed | sortBed -faidx /scicore/home/schiera/gizevo30/projects/cichlids_2/genome/GCF_001858045.1_ASM185804v2_genomic_edit.chrs -i stdin | mergeBed -i stdin > cohort_geno_gathered.allMasksMerged.bed

# Then index it? needed for VariantFiltration/SelectVariants
~/gatk-4.2.4.1/gatk IndexFeatureFile -F cohort_geno_gathered.allMasksMerged.bed


## Now use this to SelectVariants for downstream PGLS analysis!!!!!!


# OK running step10 seemed to work, but I think it didn't - likely making the index for the bed file didn't work, and actually screwed up the bed file
# Will have to remake the combined BED file, then re-index it

## Ahh, I didn't load BEDTools, so it was never merged
## Works now, smaller file comes out, goes from 204G to 151G


## Personal token for git expires 17/04/2022
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

## Hmmm, it's a bit weird, seems like maybe the depth filter didn't work properly??
## This is also the one that was weird to sort, and seems to be in a different order then the rest of them - and is potentially not be added to the allMasks
## OK NM, the depth filter acts on the total depth (across all individuals), and not the individual samples (duh!)


# OK, I really have no idea how to do the gwas, but I can find some packages to do regular (without correcting for phylogeny), maybe I can do this ahead of meeting with Milan
# Also, can use vcftools to filter by missing data

 module load VCFtools/0.1.16-GCC-10.3.0
 vcftools --gzvcf cohort_genotyped_whole_hardfiltered.g.vcf.gz --max-missing 0.2 --out cohort_genotyped_whole_hardfiltered_2.g.vcf.gz


###################################################################################################
###################################################################################################

# OK First step is to remove multi-allelic sites (and keep all bi-allelic sites), INDELS, and singletons in addition to the above filters
# 1) Multi-allelic should be a minor fraction, and are complicated/impossible to GWAS
# 2) INDELs tend to be messy, but in theory could be done in similar manner, but are ignored by Milan
# 3) Singletons (only 1 or 2 species has allele) cannot reliably be GWAS'd either
# This will reduce the file size considerably, but he also runs the GWAS per chromosome, ignoring unplaced scaffolds


# This is subsetting the VCF to keep only balletic SNPs and excluding singletons: 
# bcftools view --min-ac 2:minor -m2 -M2 --exclude-types indels -O z -o TanganyikaGWAS_region${i}_SelectedVariants_variableSnpsBiallelicNoSingletons.vcf.gz TanganyikaGWAS_region${I}_SelectedVariants.vcf.gz

# The following subsets the vcf removing 1-3 of the above
module load BCFtools/1.12-GCC-10.3.0

bcftools view --max-ac 218 --min-ac 2 -m2 -M2 --exclude-types indels -O z -o cohort_db_geno.hardfiltered.SNPS.biallelic.NoSingletons.g.vcf.gz cohort_db_geno.hardfiltered.g.vcf.gz


# The below should be done as an array job
# This is subsetting a whole-genome VCF to get per-chromosome VCFs
# This queries the list of chromosomes, with the unplaced one last

# comma separated df with rows, samples, interval
file_list="/scicore/home/schiera/gizevo30/projects/cichlids_2/genome/GCF_001858045.1_ASM185804v2_genomic_edit.chrs"

# this is the second column of index_array_40x.csv
INTERVAL=`sed -n "$SLURM_ARRAY_TASK_ID"p "${file_list}" | cut -f 1 -d ','`

bcftools view -r $INTERVAL --threads 8 -O z -o cohort_db_geno_${INTERVAL}.hardfiltered.SNPS.biallelic.NoSingletons.g.vcf.gz cohort_db_geno.hardfiltered.SNPS.biallelic.NoSingletons.g.vcf.gz




###################################################################################################

## OK next steps are to estimate allel frequencies using a Hardy-weinberg equilibrium equation
## taking into account the quality of the genome call and the depth (I think). This uses Milan's
## custom scripts/programs in the 'evo' package on github

# You need to install “evo”:
# https://github.com/millanek/evo 
# It has one dependency - the Boost library: module spider boost; module load boost/VER; 
module load Boost/1.76.0-GCC-10.3.0
cd evo; make


# This is estimating allele frequencies per-species from genotype likelihoods:
for i in {65..87}; do SUBMIT_JOB_(1GB mem) evo alleleFreq -n TanganyikaGWAS_wOreTan_NC_0319${i}_fromProbabilities --use-genotype-probabilities TanganyikaGWAS_wOreTan_NC_0319${i}_SelectedVariants_variableSnpsBiallelicNoSingletons.vcf.gz medians_for_GWAS_withoutfilter_speciesCodesAndGenomeIDs_wOretan.txt; done

# This should also be done as an array
# I'm not sure what the 'medians' file is, will have to email Milan as it is also not in the -h for alleleFreq

# comma separated df with rows, samples, interval
file_list="/scicore/home/schiera/gizevo30/projects/cichlids_2/genome/GCF_001858045.1_ASM185804v2_genomic_edit.chrs"

# this is the second column of index_array_40x.csv
INTERVAL=`sed -n "$SLURM_ARRAY_TASK_ID"p "${file_list}" | cut -f 1 -d ','`

~/evo/Build/evo alleleFreq -n cohort_db_geno_${INTERVAL}_fromProbabilities --use-genotype-probabilities cohort_db_geno_${INTERVAL}.hardfiltered.SNPS.biallelic.NoSingletons.g.vcf.gz medians_for_GWAS_withoutfilter_speciesCodesAndGenomeIDs.txt

# Errors running the above:
# /scicore/home/schiera/gizevo30/evo/Build/evo: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /scicore/home/schiera/gizevo30/evo/Build/evo)
# /scicore/home/schiera/gizevo30/evo/Build/evo: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by /scicore/home/schiera/gizevo30/evo/Build/evo)
# error is fixed by loading the Boost libraries in the script
# now get:
There are 0 populations
/var/lib/slurm/slurmd/job41803333/slurm_script: line 51: 39869 Segmentation fault      (core dumped) ~/evo/Build/evo alleleFreq -n test_fromProbabilities --use-genotype-probabilities test.g.vcf.gz medians_for_GWAS_withoutfilter_speciesCodesAndGenomeIDs.txt

# I think its because I'm missing some indexing file with sample/species names to match with the vcf

###################################################################################################

## Final steps!! Run the GWAS and the PGLS-GWAS!
## These run per chromosome, on the allele frequencies files generated above
## They run two Rscript files, which do the linear modelling and the PGLS for association of each allele with a phenotype
## Will have to modify the R scripts, but should be easy-peasy.
## They take awhile to run apparently (especially the PGLS)

module load R/4.0.3-foss-2018b

# comma separated df with rows, samples, interval
file_list="/scicore/home/schiera/gizevo30/projects/cichlids_2/genome/GCF_001858045.1_ASM185804v2_genomic_edit.chrs"

# this is the second column of index_array_40x.csv
INTERVAL=`sed -n "$SLURM_ARRAY_TASK_ID"p "${file_list}" | cut -f 1 -d ','`

Rscript GWASrun.R [argument 1 is the alleleFreq table outputed by evo]

Rscript PGLSrun.R [argument 1 is the alleleFreq table outputed by evo]

# Milan ran these with 40Gb and 60Gb of memory respectively 

# standard linear model based GWAS
for i in {65..87}; do bsub -G rdgroup -e getDP_e_%J -o getDP_o_%J -R'select[mem>40000] rusage[mem=40000]' -M40000 /software/R-4.0.3/bin/Rscript GWASrun.R medians_for_GWAS_withoutfilter_speciesCodesAndGenomeIDs_wOretan_TanganyikaGWAS_wOreTan_NC_0319${i}_fromProbabilities_AF.txt.gz; done

# PGLS-based GWAS - explicitly correcting for phylogeny, but not gene-flow
for i in {65..87}; do bsub -G rdgroup -e getDP_e_%J -o getDP_o_%J -q long -R'select[mem>60000] rusage[mem=60000]' -M60000 /software/R-4.0.3/bin/Rscript PGLSrun.R medians_for_GWAS_withoutfilter_speciesCodesAndGenomeIDs_wOretan_TanganyikaGWAS_wOreTan_NC_0319${i}_fromProbabilities_AF.txt.gz; done 



























